---
output:
  bookdown::word_document2: 
    fig_caption: yes
  fontsize: 12pt
  sansfont: Liberation Sans
  mainfont: Liberation Sans
  classoption: twocolumn
  #  reference_docx: Y:/Projects/GB_time_area_closure_SPERA/Drafts/VMS_closure_paper/word_template.docx
  bookdown::html_document2: default
  bookdown::pdf_document2:
      keep_tex: yes
      number_sections: no      
      toc: no
bibliography: Y:/Zotero/MAR_SABHU.bib
csl: Y:/Zotero/styles/canadian-journal-of-fisheries-and-aquatic-sciences.csl
title: Seasonal and Long term variability in species distribution of Atlantic cod (*Gadus morhua*) and Yellowtail Flounder (*Limanda ferruginea*) on Georges Bank
author: Keith D.M.^a,b^,  Sameoto J.A.^a^, Keyser F.M.^a^, Andrushchenko I.^a^
date:  ^a^ Fisheries and Oceans Canada 
       ^b^ Dalhousie University 
       
abstract: Sustainably managing marine fisheries has long been recognized as a global priority which has proven difficult to achieve.  The reasons sustainable fisheries management goals have not been achieved include various socio-economic, political, and scientific factors.  Scientifically, one of the major challenges has been understanding how environmental factors influence both the spatial and temporal dynamics of a stock. Fisheries science has spent a great deal of effort collecting data, both biological and environmental, which are inherently spatial and temporal in nature. Computational and statistical limitations have resulted in science products which do not fully utilize the spatio-temporal information contained in these data and tend to treat stocks as homogeneous entities.  Fortunately, computational advances coupled with more accessible statistical methods has resulted in new methodologies which can harness the spatio-temporal information contained in these fisheries data.  Here we develop temporally variable species distribution models for yellowtail flounder (*Limanda ferruginea*) and Atlantic cod (*Gadus morhua*) on Georges Bank (GB) using a suite of static environmental covariates and presence-absence information from groundfish trawl surveys in Canada and the United States.  These models indicate there are both seasonal and long term shifts in the distribution of both species.  The average sea surface temperature (SST; average from 1997-2008) and depth were significant predictors of the distribution of both species throughout the year.  Significant shifts in the distribution of both species occurs relatively frequently, with the distribution of cod observed to differ approximately every 5 years, while the Yellowtail distribution appears to fluctuate at least every 3 years. The core areas for both species shifts to the north and east throughout the study period.  Much of this shift is due to the loss of the species from southern and western portions of GB.  The seasonal distribution of cod and yellowtail are relatively consistent throughout the late winter and spring, while in the fall the distribution of cod shifts towards the edge of the bank. For cod there has been a substainal decline in core area within the United States waters on Georges Bank while there has been little change in Canadian waters.  In U.S. waters the yellowtail core area declined rapidly in the late 1970s and early 1980s, but rebounded rapidly in the 1990s and early 2000s, while the core area was unchanged or slowly increased in Canadian waters over this time. These trends have resulted in an increase in the proportion of both stocks in Canadian waters in recent years. The models for both stocks were also relatively successful at predicting the likely location of the stock up to 3 years into the future, in addtion the simplified models which only use the random field for prediction performed as well as the models that included environmental covariates. Here we show how these models are able to provide novel insights into both seasonal and inter-annual variability in species distributions even without the use of environmental covariates. The incorporation of spatial information into science advice will improve our ability to sustainably manage these stocks.  

---


<!-- Bring in the data + figures   -->
```{r echo=F, include=F, paged.print=FALSE,cache =T}
##### Bring in the data and functions
library(readxl)
library(xtable)
library(pander)
library(png)
library(PBSmapping)
library(lubridate)
library(ggplot2)
library(dplyr)
library(tidyr)
library(betareg)
library(MASS)
library(tidyverse)
library(mgcv)
library(boot)
library(cowplot)
library(sf)
library(sp)
library(RCurl)
library(units)
library(nngeo)
library(data.table)
library(ggthemes)
library(caret)
library(concaveman)
library(rosettafish)


# Bring in our friend pectinid
eval(parse(text =getURL("https://raw.githubusercontent.com/Dave-Keith/Assessment_fns/master/Maps/pectinid_projector_sf.R",ssl.verifypeer = FALSE)))
eval(parse(text =getURL("https://raw.githubusercontent.com/Dave-Keith/Assessment_fns/master/Maps/convert_inla_mesh_to_sf.R",ssl.verifypeer = FALSE)))
eval(parse(text =getURL("https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/centre_of_gravity.R",ssl.verifypeer = FALSE)))
eval(parse(text =getURL("https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/add_alpha_function.R",ssl.verifypeer = FALSE)))

#eval(parse(text =getURL("https://raw.githubusercontent.com/Mar-scal/Assessment_fns/master/Maps/convert_inla_mesh_to_sf.R",ssl.verifypeer = FALSE)))
#source("D:/Github/Offshore/Assessment_fns/DK/Maps/convert_inla_mesh_to_sf.R")
#source("D:/Github/Offshore/Assessment_fns/DK/Maps/pectinid_projector_sf.R")
# Here's a little custom function that you can use to set breakpoints in a facet plot, this one is set up the make Depth and SST's look nice
# used in combo with scale_x_continuous() in ggplot
breaks_fun <- function(x)  if (max(x) > 15) { seq(0,300,50) } else { seq(8, 14, 0.5) }
factor.2.number <- function(x) {as.numeric(levels(x))[x]} # My friend factor.2.number

# Just so this code is easily portable over to our eventual Res Doc..
french = F
#direct.proj <- "Y:/Projects/GB_time_area_closure_SPERA/" 
direct.proj <- "D:/Github/Paper_2_SDMs/"; direct.tmp <- direct.proj
# The prediction prop function
source(paste0(direct.proj,"scripts/predicted_prob_time_series_function.R"))

# Some crap we need to load
load(paste0(direct.proj,"Data/SST_and_Depth_covariates_and_boundary_for_prediction.RData"))
load(paste0(direct.proj,"Data/INLA_mesh_input_data.RData"))
load(paste0(direct.proj,"Data/INLA_meshes.RData"))
load(paste0(direct.proj,"data/Depth_SST_and_Sed_on_GB.RData"))
#load(paste0(direct.proj,"data/Prediction_fields_all_models.RData"))
load(paste0(direct.proj,"data/NEW_prediction_fields.RData"))
load(paste0(direct.proj,"data/Prediction_mesh.RData"))
load(paste0(direct.proj,"data/All_model_covariate_fits.RData"))
load(paste0(direct.proj,"data/INLA_5_fold_cross_valiation_pred_error_and_residual.RData"))
load(paste0(direct.proj,"data/INLA_2017_2019_NEW_prediction_error_summary.RData"))
load(paste0(direct.proj,"data/Gini_results.RData"))

# The meta data 
table_1 <- read_xlsx(paste0(direct.proj,"Data/enviro_data_table.xlsx"))
# This contains all the WAIC and DIC model selection diagnostics + some plots of these, see Step 6b for what was done here.
load(paste0(direct.proj,"data/model_diagnostics_for_papers.RData"))
direct.proj <-  direct.tmp 
# I think this is what I want for this, I think...
# Grab the coorindates for CA1, CA2, and the cod/yellowtail closures which are inside this other_boundaries zip file.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/other_boundaries/other_boundaries.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# Now grab the individual shape files I want.
CA1 <- st_read(dsn = paste0(temp2,"/CA1.shp"))
CA1 <- st_transform(CA1,crs = 32619)
CA2 <- st_read(dsn = paste0(temp2,"/CA2.shp"))
CA2 <- st_transform(CA2,crs = 32619)
yt.closures <- st_read(dsn =paste0(temp2,"/yt_closures.shp"))
cod.closures <- st_read(dsn =paste0(temp2,"/cod_closures.shp"))

# Now grab the coordinates for the survey boundaries for GBa and gbb, inside of the survey_boundaries zip.
temp <- tempfile()
# Download this to there
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/survey_boundaries/survey_boundaries.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# And we get GBa and GBb from there
gba.surv <- st_read(dsn = paste0(temp2,"/GBa.shp"))
gbb.surv <- st_read(dsn = paste0(temp2,"/GBb.shp"))
# Make an 'all of gb' sf object
gb.surv <- st_union(gba.surv,gbb.surv)
# This removes holes, still have one small artifact out to the east, but it matters little...
gb.surv <- nngeo::st_remove_holes(gb.surv)
gb.surv <- st_transform(gb.surv,crs= 32619)
scal.tot.area <- st_area(gb.surv) %>% set_units("km^2")
# Put all the closures together so we can easily plot them.
all.closures <- c(st_geometry(CA1),st_geometry(CA2),st_geometry(yt.closures),st_geometry(cod.closures))

# The survey data, in sf form
dat.sf <- st_as_sf(dat.final, coords = c('lon','lat'),crs = 4326,remove=F)
dat.sf <- st_transform(dat.sf,crs = 32619)
##### Done with data loading... Set some variables for the rest of the show..


# Don't use scientific notation please!!
options(scipen=999)
# Set a nice theme for the ggplots unless I override
theme_set(theme_few(base_size = 12))
# Function in case you need it for transforming propotion data to not have 0's and 1's.  
beta.transform <- function(dat,s=0.5)  (dat*(length(dat)-1) + s) / length(dat)
# Decide what you want "Hi probablity" to be for this analysis..
hi.prob <- 0.75

# I need to make the mesh.grid a nicer sf object, unclear yet to me why this is helpful...
tst <- as_Spatial(mesh.grid)
mesh.grid <- st_as_sf(tst)

# Now I want to make the prediction field from the new prediction models
mod.names <- names(pred.output.pred)
n.mods <- length(mod.names)
pred.res <- NULL
for(i in 1:n.mods)
{
  res <- pred.output.pred[[mod.names[i]]]
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_3)))
  {
    n.eras <- length(unique(res$years_5))
    eras <- factor.2.number(unique(res$years_5))
  } # End if loop
  
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_5)))
  {
    n.eras <- length(unique(res$years_3))
    eras <- factor.2.number(unique(res$years_3))
  } # end if loop
  
  res <- st_as_sf(res,coords = c("X","Y"), crs = st_crs(mesh.grid),remove = F)

    # Now for some reason my prediction grid doesn't quite line up with my prediciton mesh, so clip the mesh to match
  res <- st_join(mesh.grid,res)
  
  for(n in min(eras):max(eras))
  {
    if(all(is.na(res$years_3)))
    {
    yrs <- paste0(substr(dat.final %>% filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                  substr(dat.final %>% dplyr::filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
    if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
    res$yrs[res$years_5==n] <- yrs
    }
    
    if(all(is.na(res$years_5)))
    {
      yrs <- paste0(substr(dat.final %>% filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                    substr(dat.final %>% dplyr::filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
      if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
      res$yrs[res$years_3==n] <- yrs
    }
    
  } # end   for(n in min(eras):max(eras))

  res <- res[!is.na(res$yrs),]
  # So calculating area is smart using that set units, though they are all idenitcal...
  res$area <- res %>% st_area() %>% set_units("km^2")
  res <- res %>% dplyr::filter(pred >= 0) # THIS IS SUPER IMPORTANT!!  WHAT IS THE probability we are looking at for this.
  pred.res[[mod.names[i]]] <- res
} # end for (i) loop

# This is the thing I need to make the prediction plots and also for the COG and area calculations.
pred.res <- do.call("rbind",pred.res) # This is a useful general purpose object I want



################################
###  Figures for the paper  ###
################################

# Lets get a basemap set up for the rest of the show.
bp.zoom <- pecjector(c_sys = 32619,area = list(x=c(580000,780000), y = c(4530000,4680000),crs = 32619),add_layer = list(land='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),plot=F) #+ theme_map()

# Same but with bathy underlain
bp.bathy <-  pecjector(area="GOM",plot=F,repo = 'github',add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl',bathy = c(10,'s',200)),c_sys = 32619,buffer = 0.2) + theme_map()

# Maybe I want this sometime.
# bp.closures <- bp.bathy + geom_sf(data= CA1,fill = NA,color = 'red',size=1) + 
#                           geom_sf(data= CA2,fill = NA,color = 'green',size=1) +
#                           geom_sf(data= yt.closures, fill= NA,color = 'blue',size=1) + 
#                           geom_sf(data=cod.closures, fill= NA,color = 'black',size=1) 
# A figure providing a general overview of the area....

# #  A nice clean polygon of the core of the GB area we want to deal with here.
 clp.poly <- st_as_sf(data.frame(X = c(508000,508000,900000,650000,600000,550000),
                                 Y=c(4540000,4350000,4674000,4674000,4661000,4622000),ID=1),coords = c("X","Y"),crs= 32619)
 clp.poly <- st_cast(st_combine(clp.poly),"POLYGON")
# # Now use the bigger clp with this other clip to get a nice clipped GB area...
 clp.pred <- st_intersection(clp,clp.poly)

#####
# Figure Overview of the area, don't need to run this every time, if I want to change something uncomment the below
#####
labs <- st_as_sf(data.frame(X =c(600000, 680000), Y = c(4700000,4700000),lab = c("U.S.","Canada")),coords = c("X","Y"),crs=32619)
 
plt.over <- bp.bathy + geom_sf(data = clp.pred,fill=NA,size = 1.5, color='orange') + 
                       geom_sf(data= dat.sf,alpha = 0.25,shape=19,size=0.25, fill='black',color='black')+
                       geom_sf(data = CA1,fill = NA,size=1.25,color='blue') + geom_sf(data = CA2,fill = NA,size=1.25,color = 'white') + 
                       geom_sf(data = yt.closures,size=0.5,fill = 'lightgrey', alpha = 0.1,color = 'gold') + 
                       geom_sf(data = cod.closures,size=0.5,fill = 'lightgrey', alpha = 0.1,color = 'gold') + 
                       geom_sf_label(data = labs,aes(label = lab),parse=T)  
   
 
save_plot(paste0(direct.proj,"Results/Figures/GB_overview.png"),plt.over,base_width =6,base_height =8,units='in',dpi=300)
save_plot(paste0(direct.proj,"Results/Figures/GB_overview.tiff"),plt.over,base_width =6,base_height =8,units='in',dpi=300)
over.plt <- paste0(direct.proj,"Results/Figures/GB_overview.png")

#####
# Figure MESH don't need to run this every time, if I want to change something uncomment the below
#####

# I'm gonna need to plot my mesh on the nice bp object
mesh.gf$crs <- crs("+init=epsg:32619") 
# THis is a very minor tweak on a custom function from Finn Lindgren https://groups.google.com/forum/#!topic/r-inla-discussion-group/z1n1exlZrKM
mesh.sf <- inla.mesh2sf(mesh.gf)

#plt.mesh <- pecjector(area = "GOM",buffer=0.4, c_sys = 32619,plot=F,
#                      add_layer = list(land ='grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),
#                      add_custom = list(obj = mesh.sf$triangles, size=0.5,color= 'grey30')) + 
#                      theme_map()
#save_plot(paste0(direct.proj,"Results/Figures/mesh.tiff"),plt.mesh,base_width =8,base_height =8,units='in')
#save_plot(paste0(direct.proj,"Results/Figures/mesh.png"),plt.mesh,base_width =8,base_height =8,units='in')
mesh.plt <- paste0(direct.proj,"Results/Figures/mesh.png")

#####
# Figure SST and Depth Mapping don't need to run this every time, if I want to change something uncomment the below
#####
# Perhaps I need Sed num in here now...
#Depth and SST Maps - Commented out as these should be basically static plots
#First, I need a map showing the spatial distribution of the SST and Depth
#Don't run this unless we have to as these are pretty big, just want to load the object from this which is already saved.
# And subset the sst and depth data to this.
# sst.gb <- st_intersection(sst.gb,clp.pred)
# depth.gb <- st_intersection(depth.gb,clp.pred)
# sed.gb <- st_intersection(sed.gb,clp.pred)
# # # So for both species depth patterns over 150 meters are pretty flat, so I can just lump them altogether so we can see the key depth bits
#  depth.gb$layer[depth.gb$layer <= -150] <- -150
# 
# 
# sed.map <- pecjector(area = list(x=c(405000,790000), y = c(4400000,4800000),crs = 32619), c_sys = 32619,
#                       add_layer = list(land = 'grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),plot=F, legend =T,
#                       add_custom = list(obj = sed.gb,
#                                    scale = list(scale ='d',palette = viridis::cividis(3,begin=0.25,end=0.85,direction = -1),leg.name="")))
# 
# sst.map <- pecjector(area = list(x=c(405000,790000), y = c(4400000,4800000),crs = 32619), c_sys = 32619,
#                       add_layer = list(land = 'grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),plot=F, legend =T,
#                       add_custom = list(obj = sst.gb,
#                                    scale = list(scale ='c',palette = pals::coolwarm(25),breaks = 9:15,leg.name="SST (°C)")))
# depth.map <- pecjector(area = list(x=c(405000,790000), y = c(4400000,4800000),crs = 32619), c_sys = 32619,
#                       add_layer = list(land = 'grey',eez = 'eez',nafo = 'main',scale.bar = 'tl'),plot=F, legend =T,
#                       add_custom = list(obj = depth.gb,
#                                    scale = list(scale ='c',palette = rev(pals::brewer.blues(25)),breaks = seq(0,-150,by=-25),leg.name="Depth (m)")))
# # # Plotting this beast is really slow b/c the depth data are so fine scale
# 
# # Now combine these two figures and save it, because the save is so slow I've turned it off after making this figure the first time
# # If you want to remake it uncomment the saves below
# p.covars <- plot_grid(sst.map, depth.map,sed.map,align = "v", nrow = 3)
# save_plot(paste0(direct.proj,"Results/Figures/depth_sst_sed_fields.tiff"),p.covars,base_width =8,base_height =12,units='in')
# save_plot(paste0(direct.proj,"Results/Figures/depth_sst_sed_fields.png"),p.covars,base_width =8,base_height =12,units='in')
sst_depth_spatial.plt <- paste0(direct.proj,"Results/Figures/depth_sst_sed_fields.png")

#####
# Figure Model Selection with 1 FE.
#####
# First I think we need to discuss the model selection and show some of those plots
#plt.waic.fe <- plt.waic.fe +  theme_few(base_size = 12)
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_single_fe_waic.tiff"),plt.waic.fe,base_width =8,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_single_waic.png"),plt.waic.fe,base_width =8,base_height =8,units='in')
diag.waic.single.fe.plt <- paste0(direct.proj,"Results/Figures/Diagnostics_single_waic.png")

#####
# Figure Model Selection step 2 with multiple FE's using the 10 random field
#####
# First I think we need to discuss the model selection and show some of those plots
# Possible plots here include these, I suspect a few of these might be supplements or I do like pointing folks to my
#plt.waic.10 
# Github repo where you can run the shiny app and look at the output rather than a big boring appendix.
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_2_covars_fe_waic.tiff"),plt.waic.10,base_width =8,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_2_covars_fe_waic.png"),plt.waic.10,base_width =8,base_height =8,units='in')
diag.waic.2.covars.fe.plt <- paste0(direct.proj,"Results/Figures/Diagnostics_2_covars_fe_waic.png")

#####
# Figure Model Selection step 3 with most complex models, using 5 year random field
#####
# First I think we need to discuss the model selection and show some of those plots
# Possible plots here include these, I suspect a few of these might be supplements or I do like pointing folks to my
plt.waic.5.cod<- plt.waic.5.cod +  theme_few(base_size = 12) + xlab("")
plt.waic.5.yt<- plt.waic.5.yt +  theme_few(base_size = 12)
plt.waic.5 <- plot_grid(plt.waic.5.cod,plt.waic.5.yt,nrow=2)
# Github repo where you can run the shiny app and look at the output rather than a big boring appendix.
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_3_covars_fe_waic.tiff"),plt.waic.5,base_width =8,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_3_covars_fe_waic.png"),plt.waic.5,base_width =8,base_height =8,units='in')
diag.waic.3.covars.fe.plt <- paste0(direct.proj,"Results/Figures/Diagnostics_3_covars_fe_waic.png")

####
# Figure Model Selection for the Random fields
####
plt.cod.waic.rf <-  plt.cod.waic.rf +  theme_few(base_size = 12) + xlab("")
plt.yt.5.10.waic.rf <-  plt.yt.5.10.waic.rf +  theme_few(base_size = 12)
plt.yt.3.5.waic.rf <-  plt.yt.3.5.waic.rf +  theme_few(base_size = 12)
plt.rf.waic <- plot_grid(plt.cod.waic.rf,plt.yt.5.10.waic.rf,plt.yt.3.5.waic.rf,nrow=3)
# # Github repo where you can run the shiny app and look at the output rather than a big boring appendix.
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_rf_waic.tiff"),plt.rf.waic,base_width =10,base_height =12,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Diagnostics_rf_waic.png"),plt.rf.waic,base_width =10,base_height =12,units='in')
diag.waic.rf.plt <- paste0(direct.proj,"Results/Figures/Diagnostics_rf_waic.png")


# Then I think we need to show the depth and sst relationships for each model and survey, 
####
# Figure Cod Now we show the cod results, put depth and SST on the same figure. For cod we use the 5 year field and SST + Dep model
####

# Get the data for SST and Depth for each model.
dat.winter <- dat.final %>% dplyr::filter(survey == "RV")
dat.winter$depth_log <- log(-dat.winter$comldepth)
dat.winter$depth_cen <-  dat.winter$depth_log - mean(dat.winter$depth_log) 
dat.winter$sst_avg_cen <- scale(dat.winter$sst_avg)
# Spring survey data...
dat.spring <- dat.final %>% dplyr::filter(survey == "nmfs-spring")
dat.spring$depth_log <- log(-dat.spring$comldepth)
dat.spring$depth_cen <-  dat.spring$depth_log - mean(dat.spring$depth_log) 
dat.spring$sst_avg_cen <- scale(dat.spring$sst_avg)
# Fall survey data
dat.fall <- dat.final %>% dplyr::filter(survey == "nmfs-fall")
dat.fall$depth_log <- log(-dat.fall$comldepth)
dat.fall$depth_cen <-  dat.fall$depth_log - mean(dat.fall$depth_log) 
dat.fall$sst_avg_cen <- scale(dat.fall$sst_avg)      
## Now get the Intercept, depth terms, and sst terms from your model as appropriate
int.cod.winter <- all.mod.fixed[["cod_PA RV survey model.depth.sst_st_5"]]
depth.cod.winter <- all.mod.depth[["cod_PA RV survey model.depth.sst_st_5"]]
sst.cod.winter <- all.mod.sst[["cod_PA RV survey model.depth.sst_st_5"]]
depth.cod.winter$response <- inv.logit(depth.cod.winter$mean + int.cod.winter$mean[1])
depth.cod.winter$UCI <- inv.logit(depth.cod.winter$`0.975quant` + int.cod.winter$mean[1])
depth.cod.winter$LCI <- inv.logit(depth.cod.winter$`0.025quant` + int.cod.winter$mean[1])
depth.cod.winter$covar <- exp(depth.cod.winter$ID + mean(dat.winter$depth_log))
depth.cod.winter$survey <- "Winter"
depth.cod.winter$fe <- "Depth"
sst.cod.winter$response <- inv.logit(sst.cod.winter$mean + int.cod.winter$mean[1])
sst.cod.winter$UCI <- inv.logit(sst.cod.winter$`0.975quant` + int.cod.winter$mean[1])
sst.cod.winter$LCI <- inv.logit(sst.cod.winter$`0.025quant` + int.cod.winter$mean[1])
sst.cod.winter$covar <- sst.cod.winter$ID* attr(dat.winter$sst_avg_cen,"scaled:scale") + attr(dat.winter$sst_avg_cen,"scaled:center")
sst.cod.winter$survey <- "Winter"
sst.cod.winter$fe <- "SST"
# spring survey
int.cod.spring <- all.mod.fixed[["cod_PA nmfs-spring survey model.depth.sst_st_5"]]
depth.cod.spring <- all.mod.depth[["cod_PA nmfs-spring survey model.depth.sst_st_5"]]
sst.cod.spring <- all.mod.sst[["cod_PA nmfs-spring survey model.depth.sst_st_5"]]
depth.cod.spring$response <- inv.logit(depth.cod.spring$mean + int.cod.spring$mean[1])
depth.cod.spring$UCI <- inv.logit(depth.cod.spring$`0.975quant` + int.cod.spring$mean[1])
depth.cod.spring$LCI <- inv.logit(depth.cod.spring$`0.025quant` + int.cod.spring$mean[1])
depth.cod.spring$covar <- exp(depth.cod.spring$ID + mean(dat.spring$depth_log))
depth.cod.spring$survey <- "Spring"
depth.cod.spring$fe <- "Depth"
sst.cod.spring$response <- inv.logit(sst.cod.spring$mean + int.cod.spring$mean[1])
sst.cod.spring$UCI <- inv.logit(sst.cod.spring$`0.975quant` + int.cod.spring$mean[1])
sst.cod.spring$LCI <- inv.logit(sst.cod.spring$`0.025quant` + int.cod.spring$mean[1])
sst.cod.spring$covar <- sst.cod.spring$ID* attr(dat.spring$sst_avg_cen,"scaled:scale") + attr(dat.spring$sst_avg_cen,"scaled:center")
sst.cod.spring$survey <- "Spring"
sst.cod.spring$fe <- "SST"
# Fall survey
int.cod.fall <- all.mod.fixed[["cod_PA nmfs-fall survey model.depth.sst_st_5"]]
depth.cod.fall <- all.mod.depth[["cod_PA nmfs-fall survey model.depth.sst_st_5"]]
sst.cod.fall <- all.mod.sst[["cod_PA nmfs-fall survey model.depth.sst_st_5"]]
depth.cod.fall$response <- inv.logit(depth.cod.fall$mean + int.cod.fall$mean[1])
depth.cod.fall$UCI <- inv.logit(depth.cod.fall$`0.975quant` + int.cod.fall$mean[1])
depth.cod.fall$LCI <- inv.logit(depth.cod.fall$`0.025quant` + int.cod.fall$mean[1])
depth.cod.fall$covar <- exp(depth.cod.fall$ID + mean(dat.fall$depth_log))
depth.cod.fall$survey <- "Fall"
depth.cod.fall$fe <- "Depth"
sst.cod.fall$response <- inv.logit(sst.cod.fall$mean + int.cod.fall$mean[1])
sst.cod.fall$UCI <- inv.logit(sst.cod.fall$`0.975quant` + int.cod.fall$mean[1])
sst.cod.fall$LCI <- inv.logit(sst.cod.fall$`0.025quant` + int.cod.fall$mean[1])
sst.cod.fall$covar <- sst.cod.fall$ID* attr(dat.fall$sst_avg_cen,"scaled:scale") + attr(dat.fall$sst_avg_cen,"scaled:center")
sst.cod.fall$survey <- "Fall"
sst.cod.fall$fe <- "SST"

# Stitch it all together into something very tidyverse.
cod.fe.res <- data.frame(response = c(depth.cod.winter$response,sst.cod.winter$response,
                                      depth.cod.spring$response,sst.cod.spring$response,
                                      depth.cod.fall$response,sst.cod.fall$response),
                         covar    = c(depth.cod.winter$covar,sst.cod.winter$covar,
                                      depth.cod.spring$covar,sst.cod.spring$covar,
                                      depth.cod.fall$covar,sst.cod.fall$covar),
                         UCI      = c(depth.cod.winter$UCI,sst.cod.winter$UCI,
                                      depth.cod.spring$UCI,sst.cod.spring$UCI,
                                      depth.cod.fall$UCI,sst.cod.fall$UCI),
                         LCI      = c(depth.cod.winter$LCI,sst.cod.winter$LCI,
                                      depth.cod.spring$LCI,sst.cod.spring$LCI,
                                      depth.cod.fall$LCI,sst.cod.fall$LCI),
                         survey   = factor(c(depth.cod.winter$survey,sst.cod.winter$survey,
                                      depth.cod.spring$survey,sst.cod.spring$survey,
                                      depth.cod.fall$survey,sst.cod.fall$survey),levels = c("Winter","Spring","Fall")),
                         fe    = c(depth.cod.winter$fe,sst.cod.winter$fe,
                                      depth.cod.spring$fe,sst.cod.spring$fe,
                                      depth.cod.fall$fe,sst.cod.fall$fe))
                       
# Let's only plot this up to a depth of 300 meters, past that isn't particularly interesting
cod.fe.res <- cod.fe.res %>% filter(covar <= 300)

# So this should be the money plot
plt.cod.fe <- ggplot(cod.fe.res) + geom_line(aes(x = covar, y = response)) + 
                     geom_ribbon(aes(x = covar,ymax = UCI,ymin = LCI),fill = 'blue',alpha=0.5)+
                     facet_wrap(~fe + survey,scales='free_x',ncol = 3,strip.position = 'top') + 
                     scale_x_continuous(breaks=breaks_fun) + 
                     xlab("") + ylab("Probability") + ylim(c(0,1))

save_plot(paste0(direct.proj,"Results/Figures/Cod_fixed_effects.tiff"),plt.cod.fe,base_width =12,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Cod_fixed_effects.png"),plt.cod.fe,base_width =12,base_height =8,units='in')
cod.fe.plt <- paste0(direct.proj,"Results/Figures/Cod_fixed_effects.png")


####
# Figure YT Now we show the Yellowtail  results, put depth and SST on the same figure.
####

int.yt.winter <- all.mod.fixed[["yt_PA RV survey model.depth.sed.sst_st_3"]]
depth.yt.winter <- all.mod.depth[["yt_PA RV survey model.depth.sed.sst_st_3"]]
sst.yt.winter <- all.mod.sst[["yt_PA RV survey model.depth.sed.sst_st_3"]]
depth.yt.winter$response <- inv.logit(depth.yt.winter$mean + int.yt.winter$mean[1])
depth.yt.winter$UCI <- inv.logit(depth.yt.winter$`0.975quant` + int.yt.winter$mean[1])
depth.yt.winter$LCI <- inv.logit(depth.yt.winter$`0.025quant` + int.yt.winter$mean[1])
depth.yt.winter$covar <- exp(depth.yt.winter$ID + mean(dat.winter$depth_log))
depth.yt.winter$survey <- "Winter"
depth.yt.winter$fe <- "Depth"
# Let's only plot this up to a depth of 300 meters, past that isn't particularly interesting, have to do this here b/c of factor in covar for yt
depth.yt.winter <- depth.yt.winter %>% filter(covar <= 300)
sst.yt.winter$response <- inv.logit(sst.yt.winter$mean + int.yt.winter$mean[1])
sst.yt.winter$UCI <- inv.logit(sst.yt.winter$`0.975quant` + int.yt.winter$mean[1])
sst.yt.winter$LCI <- inv.logit(sst.yt.winter$`0.025quant` + int.yt.winter$mean[1])
sst.yt.winter$covar <- sst.yt.winter$ID* attr(dat.winter$sst_avg_cen,"scaled:scale") + attr(dat.winter$sst_avg_cen,"scaled:center")
sst.yt.winter$survey <- "Winter"
sst.yt.winter$fe <- "SST"
int.yt.winter$response <-  c(inv.logit(int.yt.winter$mean[1]),inv.logit(int.yt.winter$mean[1] + int.yt.winter$mean[2:3]))
int.yt.winter$UCI <-  c(inv.logit(int.yt.winter$`0.975quant`[1]),inv.logit(int.yt.winter$mean[1] + int.yt.winter$`0.975quant`[2:3]))
int.yt.winter$LCI <-  c(inv.logit(int.yt.winter$`0.025quant`[1]),inv.logit(int.yt.winter$mean[1] + int.yt.winter$`0.025quant`[2:3]))
int.yt.winter$survey <- "Winter"
int.yt.winter$fe <- "Sed"
rownames(int.yt.winter) <- c("Intercept", "Gravel-Sand","Sand") # Sediment 3 is Gravel sand, and 4 is Sand from USGS metadata
# spring survey
int.yt.spring <- all.mod.fixed[["yt_PA nmfs-spring survey model.depth.sed.sst_st_3"]]
depth.yt.spring <- all.mod.depth[["yt_PA nmfs-spring survey model.depth.sed.sst_st_3"]]
sst.yt.spring <- all.mod.sst[["yt_PA nmfs-spring survey model.depth.sed.sst_st_3"]]
depth.yt.spring$response <- inv.logit(depth.yt.spring$mean + int.yt.spring$mean[1])
depth.yt.spring$UCI <- inv.logit(depth.yt.spring$`0.975quant` + int.yt.spring$mean[1])
depth.yt.spring$LCI <- inv.logit(depth.yt.spring$`0.025quant` + int.yt.spring$mean[1])
depth.yt.spring$covar <- exp(depth.yt.spring$ID + mean(dat.spring$depth_log))
depth.yt.spring$survey <- "Spring"
depth.yt.spring$fe <- "Depth"
# Let's only plot this up to a depth of 300 meters, past that isn't particularly interesting, have to do this here b/c of factor in covar for yt
depth.yt.spring <- depth.yt.spring %>% filter(covar <= 300)

sst.yt.spring$response <- inv.logit(sst.yt.spring$mean + int.yt.spring$mean[1])
sst.yt.spring$UCI <- inv.logit(sst.yt.spring$`0.975quant` + int.yt.spring$mean[1])
sst.yt.spring$LCI <- inv.logit(sst.yt.spring$`0.025quant` + int.yt.spring$mean[1])
sst.yt.spring$covar <- sst.yt.spring$ID* attr(dat.spring$sst_avg_cen,"scaled:scale") + attr(dat.spring$sst_avg_cen,"scaled:center")
sst.yt.spring$survey <- "Spring"
sst.yt.spring$fe <- "SST"
int.yt.spring$response <-  c(inv.logit(int.yt.spring$mean[1]),inv.logit(int.yt.spring$mean[1] + int.yt.spring$mean[2:3]))
int.yt.spring$UCI <-  c(inv.logit(int.yt.spring$`0.975quant`[1]),inv.logit(int.yt.spring$mean[1] + int.yt.spring$`0.975quant`[2:3]))
int.yt.spring$LCI <-  c(inv.logit(int.yt.spring$`0.025quant`[1]),inv.logit(int.yt.spring$mean[1] + int.yt.spring$`0.025quant`[2:3]))
int.yt.spring$survey <- "Spring"
int.yt.spring$fe <- "Sed"
rownames(int.yt.spring) <-  c("Intercept", "Gravel-Sand","Sand") # Sediment 3 is Gravel sand, and 4 is Sand from USGS metadata
# Fall survey
int.yt.fall <- all.mod.fixed[["yt_PA nmfs-fall survey model.depth.sed.sst_st_3"]]
depth.yt.fall <- all.mod.depth[["yt_PA nmfs-fall survey model.depth.sed.sst_st_3"]]
sst.yt.fall <- all.mod.sst[["yt_PA nmfs-fall survey model.depth.sed.sst_st_3"]]
depth.yt.fall$response <- inv.logit(depth.yt.fall$mean + int.yt.fall$mean[1])
depth.yt.fall$UCI <- inv.logit(depth.yt.fall$`0.975quant` + int.yt.fall$mean[1])
depth.yt.fall$LCI <- inv.logit(depth.yt.fall$`0.025quant` + int.yt.fall$mean[1])
depth.yt.fall$covar <- exp(depth.yt.fall$ID + mean(dat.fall$depth_log))
depth.yt.fall$survey <- "Fall"
depth.yt.fall$fe <- "Depth"
# Let's only plot this up to a depth of 300 meters, past that isn't particularly interesting, have to do this here b/c of factor in covar for yt
depth.yt.fall <- depth.yt.fall %>% filter(covar <= 300)


sst.yt.fall$response <- inv.logit(sst.yt.fall$mean + int.yt.fall$mean[1])
sst.yt.fall$UCI <- inv.logit(sst.yt.fall$`0.975quant` + int.yt.fall$mean[1])
sst.yt.fall$LCI <- inv.logit(sst.yt.fall$`0.025quant` + int.yt.fall$mean[1])
sst.yt.fall$covar <- sst.yt.fall$ID* attr(dat.fall$sst_avg_cen,"scaled:scale") + attr(dat.fall$sst_avg_cen,"scaled:center")
sst.yt.fall$survey <- "Fall"
sst.yt.fall$fe <- "SST"
int.yt.fall$response <-  c(inv.logit(int.yt.fall$mean[1]),inv.logit(int.yt.fall$mean[1] + int.yt.fall$mean[2:3]))
int.yt.fall$UCI <-  c(inv.logit(int.yt.fall$`0.975quant`[1]),inv.logit(int.yt.fall$mean[1] + int.yt.fall$`0.975quant`[2:3]))
int.yt.fall$LCI <-  c(inv.logit(int.yt.fall$`0.025quant`[1]),inv.logit(int.yt.fall$mean[1] + int.yt.fall$`0.025quant`[2:3]))
int.yt.fall$survey <- "Fall"
int.yt.fall$fe <- "Sed"
rownames(int.yt.fall) <-  c("Intercept", "Gravel-Sand","Sand") # Sediment 3 is Gravel sand, and 4 is Sand from USGS metadata

# Stitch it all together into something very tidyverse.
yt.fe.res <- data.frame(response = c(depth.yt.winter$response,sst.yt.winter$response,int.yt.winter$response,
                                      depth.yt.spring$response,sst.yt.spring$response,int.yt.spring$response,
                                      depth.yt.fall$response,sst.yt.fall$response,int.yt.fall$response),
                         covar    = c(depth.yt.winter$covar,sst.yt.winter$covar, rownames(int.yt.winter),
                                      depth.yt.spring$covar,sst.yt.spring$covar, rownames(int.yt.spring),
                                      depth.yt.fall$covar,sst.yt.fall$covar,rownames(int.yt.fall)),
                         UCI      = c(depth.yt.winter$UCI,sst.yt.winter$UCI,int.yt.winter$UCI,
                                      depth.yt.spring$UCI,sst.yt.spring$UCI,int.yt.spring$UCI,
                                      depth.yt.fall$UCI,sst.yt.fall$UCI,int.yt.fall$UCI),
                         LCI      = c(depth.yt.winter$LCI,sst.yt.winter$LCI,int.yt.winter$LCI,
                                      depth.yt.spring$LCI,sst.yt.spring$LCI,int.yt.spring$LCI,
                                      depth.yt.fall$LCI,sst.yt.fall$LCI,int.yt.fall$LCI),
                         survey   = factor(c(depth.yt.winter$survey,sst.yt.winter$survey,int.yt.winter$survey,
                                      depth.yt.spring$survey,sst.yt.spring$survey,int.yt.spring$survey,
                                      depth.yt.fall$survey,sst.yt.fall$survey,int.yt.fall$survey),levels = c("Winter","Spring","Fall")),
                         fe    = c(depth.yt.winter$fe,sst.yt.winter$fe,int.yt.winter$fe,
                                      depth.yt.spring$fe,sst.yt.spring$fe,int.yt.spring$fe,
                                      depth.yt.fall$fe,sst.yt.fall$fe,int.yt.fall$fe))
# Generally in the 60-80 range
#yt.fe.res %>% filter(response > 0.1)

plt.yt.cont.fe <- ggplot(yt.fe.res %>% filter(fe != "Sed")) + geom_line(aes(x = as.numeric(covar), y = response)) + 
                     geom_ribbon(aes(x = as.numeric(covar),ymax = UCI,ymin = LCI),fill = 'blue',alpha=0.5)+
                     facet_wrap(~fe + survey,scales='free',ncol = 3,strip.position = 'top') + 
                     scale_x_continuous(breaks=breaks_fun) + 
                     xlab("") + ylab("Probability") + ylim(c(0,0.55))

plt.yt.fact.fe <- ggplot(yt.fe.res %>% filter(fe == "Sed")) + 
                               geom_point(aes(x = factor(covar, levels = c("Intercept","Gravel-Sand","Sand")), y = response)) + 
                               geom_errorbar(aes(x = factor(covar, levels = c("Intercept","Gravel-Sand","Sand")),ymax = UCI,ymin = LCI),width=0)+
                               facet_wrap(~fe + survey,scales='free',ncol = 3,strip.position = 'top') + 
                               xlab("") + ylab("Probability") + ylim(c(0,0.55))

plt.yt.fe <- plot_grid(plt.yt.cont.fe,plt.yt.fact.fe,align = 'v',nrow=2,rel_heights = c(2,1))


save_plot(paste0(direct.proj,"Results/Figures/yt_fixed_effects.tiff"),plt.yt.fe,base_width =12,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/yt_fixed_effects.png"),plt.yt.fe,base_width =12,base_height =8,units='in')
yt.fe.plt <- paste0(direct.proj,"Results/Figures/yt_fixed_effects.png")


#####
# Figure Center of gravity of the species distributions has shifted
#####
# So here we need the predicted field for each model. I think we want to show how COG has moved for cod and yellowtail
# and have a panel for each survey, might end up being better as 2 figures, not sure yet.  But we'll want 6 cogs
# I also want to clean up the names in pred.dat (could come in handy throughout rather that using the labeller everywhere..

names.survs <- unique(pred.res$model)
pred.cog <- NULL

for(i in 1:length(names.survs))
{
  res <- pred.res %>% filter(model == names.survs[i])
  
  if(all(is.na(res$years_3)))
  {
    n.eras <- length(unique(res$years_5))
    eras <- factor.2.number(unique(res$years_5))
    res <- res[order(res$years_5),]
  } # End if loop
  
  # Easy way to pick 3 vs 5 year since I kept both columns but NA'ed the one not being used
  if(all(is.na(res$years_5)))
  {
    n.eras <- length(unique(res$years_3))
    eras <- as.numeric(unique(res$years_3))
    res <- res[order(res$years_3),]
  } # end if loop
  # Make this into an sf object
 # res <- st_as_sf(res,coords = c("X","Y"), crs = st_crs(mesh.grid),remove = F)
  # Combine the mesh into the results so we have predictions at each mesh element
  #res <- st_join(mesh.grid,res)  
  # Get the years right for each input.
  for(n in min(eras):max(eras))
  {
    if(all(is.na(res$years_3)))
    {
    yrs <- paste0(substr(dat.final %>% filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                  substr(dat.final %>% dplyr::filter(years_5 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
    if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
    res$yrs[res$years_5==n] <- yrs
    }
    
    if(all(is.na(res$years_5)))
    {
      yrs <- paste0(substr(dat.final %>% filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(min = min(year)),3,4),"-",
                    substr(dat.final %>% dplyr::filter(years_3 == n, survey == unique(res$survey)) %>% dplyr::summarise(max = max(year)),3,4))
      if(substr(yrs[1],1,2) > 30) { yrs <- paste0(19,yrs)} else {yrs <- paste0(20,yrs)}
      res$yrs[res$years_3==n] <- yrs
    }
    
  } # end   for(n in min(eras):max(eras))

  res <- res[!is.na(res$yrs),]
  # So calculating area is smart using that set units, though they are all idenitcal...
  res$area <- res %>% st_area() %>% set_units("km^2")
  res <- res %>% filter(pred >= hi.prob) # THIS IS SUPER IMPORTANT!!  WHAT IS THE probability we are looking at for this.

  cog_n_area <- as.data.table(res)[,cog.calc(X,Y,pred), by = yrs]
  cog_n_area <- cog_n_area[order(cog_n_area$yrs)]
  cog_n_area <- st_as_sf(cog_n_area,coords = c('x','y'), crs= st_crs(mesh.grid), remove=F)
  area <- res %>% group_by(yrs) %>% dplyr::summarise(area = sum(area))
  st_geometry(area) <- NULL
  cog_n_area$area <- area$area
  # This object has what we need for COG and area calcs.  
  cog_n_area$mod <- names.survs[i]
  cog_n_area$species <- res$species[1]
  cog_n_area$survey <- res$survey[1]
  pred.cog[[names.survs[i]]] <- cog_n_area
}


cog.n.area <- do.call('rbind',pred.cog)
# Make names nice...
cog.n.area$species[cog.n.area$species == "yt_PA"] <- "Yellowtail"
cog.n.area$species[cog.n.area$species == "cod_PA"] <- "Cod"
cog.n.area$survey[cog.n.area$survey == "nmfs-spring"] <- "Spring"
cog.n.area$survey[cog.n.area$survey == "nmfs-fall"] <- "Fall"
cog.n.area$survey[cog.n.area$survey == "RV"] <- "Winter"
cog.n.area$survey <- factor(cog.n.area$survey, levels = c("Winter","Spring","Fall"))
#cog.n.area$eras <- as.numeric(factor(cog.n.area$yrs, labels =1:length(unique(cog.n.area$yrs))))


plt.cog <-  bp.zoom + geom_label(data = cog.n.area,aes(x=x, y = y,label=substr(yrs,3,8)),nudge_x = -7500,nudge_y=3500,size=2) +
                  facet_wrap(~species + survey) + 
                  geom_errorbar(data = cog.n.area,aes(x= x,ymin=y - 3*se.y,ymax=y + 3*se.y),colour = "blue",width=0,size=1)  +
                  geom_errorbar(data = cog.n.area,aes(y= y,xmin=x - 3*se.x,xmax=x + 3*se.x),colour = "blue",width=0,size=1)  +
                  xlab("") + ylab("") + theme(panel.border = element_rect(colour = "black", fill=NA, size=1))


save_plot(paste0(direct.proj,"Results/Figures/center_of_gravity.tiff"),plt.cog,base_width =12,base_height =12,units='in')
save_plot(paste0(direct.proj,"Results/Figures/center_of_gravity.png"),plt.cog,base_width =12,base_height =12,units='in')
cog.plt <- paste0(direct.proj,"Results/Figures/center_of_gravity.png")

### Now how has the area changed over time...
# Things will get more complicated here as I have 3 year and 5 year fields mixed together for Yellowtail
# I'll need to make a more general axis for the x's somehow...
# I need to get the last year on here for the figure as well.
tmp <- cog.n.area[grep("16",cog.n.area$yrs),]
tmp$yrs <- "2016"
rownames(tmp) <- paste0(rownames(tmp),".1")
cog.n.area.4.plt <- bind_rows(cog.n.area,tmp)
cols <- addalpha(c("black", "blue","darkgreen"),alpha=0.5) 
plt.area <- ggplot(cog.n.area.4.plt) + geom_step(aes(x = as.numeric(substr(yrs,1,4)), y = as.numeric(area), group = survey,color= survey),lwd=1.5) +  
                                 facet_wrap(~ species , scales = 'free_x') + ylab("Area (km²)") + xlab("")+# Get the squared with Alt + 0178.. bam
                                 scale_y_continuous(breaks = seq(0,40000,2500)) + 
                                 scale_x_continuous(breaks = seq(1970,2020,5)) +
                                 scale_color_manual(values = cols)




save_plot(paste0(direct.proj,"Results/Figures/Area_ts_high.tiff"),plt.area,base_width =12,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Area_ts_high.png"),plt.area,base_width =12,base_height =8,units='in')
area.plt <- paste0(direct.proj,"Results/Figures/Area_ts_high.png")



########## Canada VS US plot #######################

temp <- tempfile()
      # Download this to the temp directory you created above
download.file("https://raw.githubusercontent.com/Mar-scal/GIS_layers/master/EEZ/EEZ.zip", temp)
# Figure out what this file was saved as
temp2 <- tempfile()
# Unzip it
unzip(zipfile=temp, exdir=temp2)
# Now read in the shapefile
eez.all <- st_read(paste0(temp2, "/EEZ.shp"))
rm(temp,temp2)
clp.eez <- st_as_sf(data.frame(X = c(-70,-70,-62.2,-62.2),Y = c(39,44,44,39)),coords = c("X","Y"),crs = 4326)
clp.eez <- st_cast(st_combine(clp.eez),"POLYGON")
eez.all <- eez.all %>% st_transform(4326)
tmp <- st_intersection(eez.all,clp.eez)
eez.can <- concaveman(tmp)
eez.can <- eez.can %>% st_transform(32619)

pf.res <- pred.res

pf.res$area <- pf.res %>% st_area() %>% set_units("km^2")
# Now who is in canada and who isn't...

pf.can <- st_intersection(pf.res,eez.can)
pf.can$country <- "Canada"
pf.us <- st_difference(pf.res,eez.can)
pf.us$country <- en2fr("USA",french)
pf.can$area <-  pf.can %>% st_area() %>% set_units("km^2") %>% as.numeric()
pf.us$area <-  pf.us %>% st_area() %>% set_units("km^2") %>% as.numeric()
pf.area <- bind_rows(pf.us, pf.can)

#pf.hi <- pf.winter.yt %>% dplyr::filter(pred >= hi.prob)
area.era.pred <- pf.area  %>% dplyr::filter(pred >= hi.prob) %>% 
                        group_by(country,model,yrs,species) %>% 
                        dplyr::summarize(tot.area = as.numeric(sum(area)))

area.era.pred$tot.area <- as.numeric(area.era.pred$tot.area)                             
area.era.pred$species[area.era.pred$species == "yt_PA"] <- en2fr("Yellowtail",french,case = 'title')
area.era.pred$species[area.era.pred$species == "cod_PA"] <- en2fr("Cod",french,case = 'title')
area.era.pred$model[grepl("RV",area.era.pred$model)] <- en2fr("Winter",french,case = 'title') 
area.era.pred$model[grepl("spring",area.era.pred$model)] <- en2fr("Spring",french,case = 'title')
area.era.pred$model[grepl("fall",area.era.pred$model)] <- en2fr("Fall",french,case = 'title')
area.era.pred$model <- factor(area.era.pred$model, levels = c(en2fr("Winter",french,case = 'title'),
                                                              en2fr("Spring",french,case = 'title'),
                                                              en2fr("Fall",french,case = 'title')))


tmp <- area.era.pred[grep("16",area.era.pred$yrs),]
tmp$yrs <- "2016"
rownames(tmp) <- paste0(rownames(tmp),".1")
area.era.pred.4.plt <- bind_rows(area.era.pred,tmp)
cols <- addalpha(c("black", "blue","darkgreen"),alpha=0.5) 

plt.area.can.vs.us <- ggplot(area.era.pred.4.plt) + geom_step(aes(x=as.numeric(substr(yrs,1,4)), y = tot.area,color = model,group=model),lwd=1.5) + facet_wrap(~species + country ) + scale_color_manual(values =cols) +  scale_x_continuous(breaks = seq(1970,2020,5)) + 
  xlab("")  +  ylab(paste0(en2fr("Area",french,case="title"), " (km²)")) + theme(legend.title = element_blank())

save_plot(paste0(direct.proj,"Results/Figures/Area_can_vs_us_ts_high.tiff"),plt.area.can.vs.us,base_width =11,base_height =7,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Area_can_vs_us_ts_high.png"),plt.area.can.vs.us,base_width =11,base_height =7,units='in')
area.can.vs.us.plt <- paste0(direct.proj,"Results/Figures/Area_can_vs_us_ts_high.png")




#####
# Figures for 5 fold cross validation and Prediction
#####
# If we had 0 predictive power our RMSE would be around this, 
# The runif is a field of random numbers between 0 and 1, while the rbinom is 50 0s and 1s with a 50-50 probabilty 
# This probability  doesn't seem to matter for RMSE calcs the runif really generates the randomness of no predictability.
null.rmse <- NA
set.seed(123)
for(i in 1:10000) null.rmse[i] <- RMSE(runif(50,0,1),rbinom(50,1,0.5)) # using 50 as this is roughly number of stations, but # doesn't actually matter.
mn.crap.rmse <- mean(null.rmse)
cols <- addalpha(c("blue", "black"),alpha=0.5) 

fold.res$model.id <- factor(fold.res$model.id, levels = c("Intercept","Depth","SST","Depth + SST"))
mn.folds <- ggplot(fold.res) + geom_point(aes(y = mn, x= model.id,colour = type),position = position_dodge(width=0.3)) + ylab("Mean Error") + xlab("")+
                               facet_wrap(~species,scales = 'free_x')+ scale_color_manual(values = cols) +
                               theme(legend.title = element_blank())

rmse.folds <- ggplot(fold.res) + geom_point(aes(y = rmse, x= model.id,colour = type),position = position_dodge(width=0.3)) + ylab("RMSE") + xlab("")+
                                 facet_wrap(~species,scales = 'free_x')  + scale_color_manual(values = cols) +
                                 geom_hline(yintercept = mn.crap.rmse,linetype = 2,color = 'red') + theme(legend.title = element_blank()) 
                                
plt.folds <- plot_grid(mn.folds,rmse.folds,nrow=2)
save_plot(paste0(direct.proj,"Results/Figures/cross_fold_validation.tiff"),plt.folds,base_width =12,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/cross_fold_validation.png"),plt.folds,base_width =12,base_height =8,units='in')
folds.plt <- paste0(direct.proj,"Results/Figures/cross_fold_validation.png")



## This is the final bit that will need tidied up with the new results, the predictions from all 6 models but 
# with an intercept comparison as well.  This figure probably looks much different!
# Now here's how well the prediction work for spawning aggregations between 2017-2019...
all.resids$type <- "Model residual"
all.resids$type[all.resids$year %in% 2017:2019] <- 'Model predicted'
all.resids$species[all.resids$species == 'yt_PA'] <- "Yellowtail"
all.resids$species[all.resids$species == 'cod_PA'] <- "Cod"
all.resids$survey <- factor(all.resids$survey,levels = c("Winter","Spring","Fall"))
all.resids$model.id[all.resids$model.id == 'intercept'] <- "No covariates"
all.resids$model.id[all.resids$model.id == 'full'] <- "Full Model"
#all.resids$line.size <- 0.5
#all.resids$line.size[all.resids$model.id == "full"] <- 0.25
pred.17.19 <- all.resids %>%  group_by(model,species,year,model.id,survey,type) %>% summarise(mn = mean(resid), rmse = RMSE(fitted,response))
pred.17.19$survey <- factor(pred.17.19$survey,levels = c("Winter","Spring","Fall"))
#pred.17.19$field[pred.17.19$field ==3] <- "3 year field"
#pred.17.19$field[pred.17.19$field ==5] <- "5 year field"
cols <- addalpha(c("blue","black"),alpha=0.5) 


plt.pred.17.19 <- ggplot(pred.17.19) + geom_line(aes(x=year,y = rmse,color = type,linetype = model.id)) + xlab("") + ylab("RMSE") + 
                                       facet_wrap(~ species +survey, scales = 'free_x') + 
                                       geom_hline(yintercept = mn.crap.rmse,linetype = 4,color = 'red') +
                                       scale_linetype_manual(name="Guide1",values= c('solid', 'dashed'))+ 
                                       scale_colour_manual(name="Guide1", values = cols) + theme_few() +theme(legend.title = element_blank()) 
#+ scale_color_manual(values = cols)# +geom_vline(aes(xintercept = 2016.5))#

save_plot(paste0(direct.proj,"Results/Figures/prediction_2017_2019.tiff"),plt.pred.17.19,base_width =12,base_height =8,units='in')
save_plot(paste0(direct.proj,"Results/Figures/prediction_2017_2019.png"),plt.pred.17.19,base_width =12,base_height =8,units='in')
pred.17.19.plt <- paste0(direct.proj,"Results/Figures/prediction_2017_2019.png")

################################
#### Gini Figures
#################################

gini.surveys$species[gini.surveys$species == "Yellowtail Flounder"] <- "Yellowtail"
gini.surveys$species[gini.surveys$species == "Atlantic Cod"] <- "Cod"

# Now the two figures, I don't think we need this first one for the paper, but is handy I think...
plt.gini.cum.prop <- ggplot(gini.surveys) + geom_line(aes(x = cum.p.area, y = cum.pbm, color = year,group = year)) + facet_wrap(~species+survey) + 
  geom_abline(slope=1,intercept =0) + xlim(c(0,1)) + ylim(c(0,1)) + 
  ylab("Cumlative Proportion of Biomass") + xlab("Cumulative Area")  + scale_color_viridis_c(option = "A")

save_plot(paste0(direct.proj,"Results/Figures/Gini_cum_prop_figures.tiff"),plt.gini.cum.prop,base_width =12,base_height =12,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Gini_cum_prop_figures.png"),plt.gini.cum.prop,base_width =12,base_height =12,units='in')
gini.cum.prop.plt <- paste0(direct.proj,"Results/Figures/Gini_cum_prop_figures.png")

cols <- addalpha(c("forestgreen", "black"),alpha=0.5) 
plt.gini.index <- ggplot(gini.surveys) + geom_line(aes(x = year, y = Gini,color=species))  + theme(legend.title = element_blank()) +
  facet_wrap(~survey) + ylim(c(0,1)) + xlab("") + ylab("Gini Index") + scale_color_manual(values = cols)

save_plot(paste0(direct.proj,"Results/Figures/Gini_index.tiff"),plt.gini.index,base_width =12,base_height =6,units='in')
save_plot(paste0(direct.proj,"Results/Figures/Gini_index.png"),plt.gini.index,base_width =12,base_height =6,units='in')
gini.index.plt <- paste0(direct.proj,"Results/Figures/Gini_index.png")

################################
###  Getting data for use in the paper  ###
################################
n.stations <- dat.final %>% group_by(survey) %>% summarise(ns = n())
n.nmfs.spring <- n.stations$ns[n.stations$survey == 'nmfs-spring']
n.rv <- n.stations$ns[n.stations$survey == 'RV']
n.nmfs.fall <- n.stations$ns[n.stations$survey == 'nmfs-fall']

# Sediment type overall...
sed.bd <- table(dat.final$SEDNUM)
per.3.4.sed <- signif(100*sum(sed.bd[2:3])/length(dat.final$SEDNUM), digits = 2)

# Get some of the cod FE numbers... hard to pick a number here give variabilty but save to say looking at these the drop occurs between 10 and 11 °C
wt.cod <- cod.fe.res %>% dplyr::filter(survey == "Winter" & fe == "SST") 
st.cod <- cod.fe.res %>% dplyr::filter(survey == "Spring" & fe == "SST") 
ft.cod <- cod.fe.res %>% dplyr::filter(survey == "Fall" & fe == "SST") 

peaks <- aggregate(response ~ survey + fe,data = cod.fe.res, FUN = function(x) which(x == max(x)))
wd.cod <- cod.fe.res %>% dplyr::filter(survey == "Winter" & fe == "Depth") 
sd.cod <- cod.fe.res %>% dplyr::filter(survey == "Spring" & fe == "Depth") 
s.dep.peak <- sd.cod$covar[peaks$response[2]]
w.dep.peak <- wd.cod$covar[peaks$response[1]]
c.dep.peak <- paste0(round(s.dep.peak,digits =0),"-",round(w.dep.peak,digits=0))

# For yellowtail pick a peak depth as well
peaks <- aggregate(response ~ survey + fe,data = yt.fe.res, FUN = function(x) which(x == max(x)))
wd.yt <- yt.fe.res %>% dplyr::filter(survey == "Winter" & fe == "Depth") 
sd.yt <- yt.fe.res %>% dplyr::filter(survey == "Spring" & fe == "Depth") 
fd.yt <- yt.fe.res %>% dplyr::filter(survey == "Fall" & fe == "Depth") 
s.dep.peak <- as.numeric(sd.yt$covar[peaks$response[2]])
w.dep.peak <- as.numeric(wd.yt$covar[peaks$response[1]])
f.dep.peak <- as.numeric(fd.yt$covar[peaks$response[3]])
# Spring and winter are the deepest so take those..
yt.dep.peak <- paste0(round(s.dep.peak,digits =0),"-",round(w.dep.peak,digits=0))

```


\twocolumn

# Introduction

Sustainable management of marine fisheries has been recognized as a critical challenge facing society in the 21^st^ century [@cbdAichiBiodiversityTargets2018]. The challenges facing sustainable fisheries management are multifaceted and include complex socio-economic, political, and scientific interactions (CITE). Environmental and ecological research has been at the heart of fisheries science for well over a century (CITE). From the early days of fisheries science it was recognized that an inability to fully account for spatial processes was potentially a serious issues to surmount (CITE). Many of the traditional fisheries methods developed, and still currently used to assess fisheries, required strong assumptions about the underlying spatial processes; during the development of these methods these assumptions were often identified as potentially problematic (CITE Ricker, BH, and maybe Carl and Ray's book). A number of methods had subsequently been developed to attempt to account for spatial processes, but computational limitations had restricted the complexity of these models (CITE - Allowing for multiple fleets was one of these).  In more recent years a flurry of computational and statistical advances have enabled the development of models in which spatial processes can be more rigorously addressed within these traditional fisheries modelling frameworks [@aeberhardReviewStateSpaceModels2018; @thorsonGeostatisticalDeltageneralizedLinear2015; @thorsonSpatialDelaydifferenceModels2015; @cadrinDefiningSpatialStructure2020a].

One of the earliest modelling frameworks developed to explicitly account for spatial patterns and processes were species distribution models with (SDMs; CITE early SDM work both terrestrial and fisheries). These models use environmental data along information about the species to map the likelihood of encountering a species across some land(sea)-scape and originated with attempts to map terrestrial plant distributions (CITE).  In the marine realm SDM's have been used to assist with the development of Marine Protect Areas (MPAs), MPA networks, Species at Risk (SAR), name some other shit (CITE).  In many cases SDM's assume that there is no temporal change in the relationship between the environment cues and the response of the species to these cues; these SDM's essentially provide a snapshot in time based on available data. To better predict future states more sophisticated SDM's frameworks have been developed in which the underlying relationships can vary both in time and space (CITE).

*I need a segue paragraph which I'm not sure what it looks like, maybe something like this?*

The collection of data in fisheries science, both biological and environmental, is often inherently spatial and temporal in nature. In the past, computational and statistical limitations have resulted in science products which do not fully utilize the spatio-temporal information contained in these data, for example, SDM applications often average temporally and the year in which the biological data is collected is not accounted for (CITE), while stock assessment methods aggregate information spatially and treat stocks as homogeneous entities (dynamic pools?? CITE).  Fortunately, the aforementioned computational advances coupled with more accessible statistical methods has resulted in new methodologies which can better harness the spatio-temporal information inherent in the data; this is especially relevant in regions in which there is a long history of high quality environmental, ecological, and fishery data collection. 

Georges Bank (GB) has been home to some of the most productive fisheries in the world for centuries and is home to a wealth of natural resources [CITE]. For this reason this area has been host to intensive scientific research programs for decades (CITE SOME STUFF).  This research has resulted in a wealth of scientific knowledge about the geology, environment, and ecology of this area (CITE SOME STUFF). Historically numerous countries had large fisheries in the region but with the expansion of territorial seas to 200 miles offshore in the late 1970's, resource exploitation (e.g. fisheries) on GB fell under the jurisdiction of Canada and the United States (CITE NAFO something). The final demarcation of the Canadian and United States territorial waters on GB were implemented with an International Court of Justice decision in 1984 (CITE). Within three years of this decision both countries had developed their own independent groundfish surveys which each covered the entirety of the bank at different times of the year.

Historically, GB was home to large groundfish fisheries including Atlantic cod (*Gadus morhua*), Atlantic haddock (*Melanogrammus aeglefinus*), Yellowtail flounder (*Limanda ferruginea*) and numerous other species (CITE). As observed throughout the northwest Atlantic, the biomass of Atlantic cod on GB declined significantly in early 1990's and there have been little evidence for recovery of this stock since this collapse [@andrushchenkoAssessmentEasternGeorges2018]. Yellowtail flounder on GB had been at low abundance on the bank since the 1970's, but evidence for a rapid recovery of this species was observed in the early 2000's and resulted in directed fisheries for this species for several years, unfortunately this recovery was short lived and the abundance of this species has been near historical lows for the last decade [@legaultStockAssessmentGeorges2018].  While the status of these two groundfish species remains poor, other groundfish stocks (e.g. Atlantic haddock) have experienced large increases in biomass over the last decade, and currently the most lucrative fishery on GB has been an invertebrate sea scallop fishery which has experienced unprecedented productivity over the last two decades [@finleyAssessmentHaddockEastern2019; CITE SABHU and DVORA].

Fisheries management bodies in both Canada and the United States have implemented measures to protect cod and yellowtail on GB.  While these measures vary between the countries, both countries have collaborated to develop a shared bycatch quota for these two species; this quota has declined substantially for both species over the last decade [@andrushchenkoAssessmentEasternGeorges2018; @legaultStockAssessmentGeorges2018]. In addition to these regulations which attempt to directly limit the removals of these two species, both countries have implemented closures. In the United States two large closed area were implemented (Closed Area I and 2) in 1994, these closures were designed to aid in the recovery of groundfish and invertebrate stocks on GB [@linkEffectsAreaClosures2005; @murawskiLargescaleClosedAreas2000].  In Canada the groundfish fishery is not permitted to fish on GB during cod spawning; since 2010 this closure has started in the 5^th^ week of the year and lasts until May 31^st^.  The Canadian Offshore Scallop Fishery (COSF) also faces restrictions on fishing during the peak spawning periods with time-area closures limiting the area this fishery can operate in February and March (cod; @dfoScallopFisheryArea2019) and June [yellowtail; @dfoScallopFisheryArea2014].

Here we use a recently developed statistical framework (CITE R-INLA) to develop spatio-temporal species distribution models for two depleted groundfish stocks on GB (Atlantic cod and Yellowtail flounder).  Our objectives were; 1) Use a suite of static environmental layers to determine whether any of these environmental data informed the distribution of either species, 2) determine whether the species distributions changed over time and if so how rapidly changes in the distributions could be observed, 3) determine whether the species distributions change seasonally using data from groundfish surveys in the winter, spring, and fall, 4) quantify the changes in core area within Canada, the U.S. and in the two largest closed areas on Georges Bank.


<!-- From a scientific perspective, disentangling how environmental, ecological, and anthropogenic factors impact the population dynamics of marine fishes is pivotal to development of sustainability strategies.  -->

<!-- Species Distribution models (SDMs) have been used for a long time in fisheries.  These models typically try to map spatial patterns in species distribution using available environmental covariates.  Without a detailed knowledge of processes underlying the spatial patterns the use of environmental covariates alone cannot fully account for spatial and temporal variability.  These environmental covariates are typically proxies for more complex unobserved(able)ed processes, and changes in these relationships are difficult to account for in these models. -->

<!-- Recent statistical advances have lead to the development of tools which can be used to develop more realistic SDMs.  These models can account for environmental covariates along with accounting for unexplained spatio-temporal variability.  These kindas of SDMs enable the model to identify the consistent environmental signal (covariates) to be estimated while also providing a statistical framework in which the unexplained spatio-temporal variability can be used to better understand spatio-temporal changes in the species distribution. -->

<!-- Tracking spatio-temporal changes facilitates the development of models which can identify consistent spatial anomalies in which the metric being measures deviates from expectation.  Tracking long-term changes improves our understanding of species shifts and provides insight into how changing environmental conditions impact the strength of the environmental correlations.  This provides a framework for predicting the impact of directed environmental change (e.g. climate change). -->


# Methods

### Study area 

Georges Bank (GB), located in the northwest Atlantic straddling the US-Canada maritime border, is a 3-150 m deep plateau that covers approximately 40,000 $km^2$ and is characterized by high primary productivity, and historically high fish abundance [@townsendNitrogenLimitationSecondary1997]. It is an eroding bank with no sediment recharge, and covered with coarse gravel and sand that provides important habitat for many species [@valentineSeaFloorEnvironment1991]. Since 1984, GB has been divided between the US and Canada and, while some collaborative management exists, the US and Canadian portions are largely managed separately (Figure \@ref(fig:Overview)). 

### Data

Survey data were obtained from the Fisheries and Oceans Canada (DFO) winter RV survey from 1987-2019 and the National Marine Fisheries Service (NMFS) spring and fall groundfish surveys from 1972-2019.  The DFO-winter survey on GB typically occurs in February and early March, the NMFS-spring survey typically occurs in April and May, while the NMFS-fall survey generally takes place between September and November.  For all surveys only tows deemed *successful* were used in this analysis.  This resulted in `r n.rv` tows from the DFO-winter survey, `r n.nmfs.spring` tows from the NMFS-spring survey, and `r n.nmfs.fall` tows from the NMFS-fall survey.

### Environmental covariates

A suite of 21 environmental variables with spatial information were obtained for this analysis (Table \@ref(tab:envrio)).  To eliminate redundant variables, variance Inflation Factors (VIFs) were calculated for all variables and any variables with VIF scores > 3 were removed.  This procedure was repeated until no variables remained with a VIF score > 3 (CITE ZUUR). Using the remaining 16 variables a Principle Component Analysis (PCA) was undertaken for each survey using the data from station locations for each survey these environmental data with the top 4 PCA components retained (these accounted for at least 80% of the variability in the data) and were included as covariates for the models that follow.

### Statistical Analysis

A Bayesian hierarchical methodology was implemented using the Integrated Nested Laplace Approximation approach available within the R Statistical Programming software R-INLA (CITE R and INLA).  In recent years R-INLA has seen a rapid increase in use to model species distributions in both within the terrestrial and marine realms (CITE SOME PAPERS). This methodology solves stochastic partial differential equations on a spatial triangulated mesh; this mesh is typically based on the data available (CITE RUE). To avoid edge effects the mesh is extended beyond the boundaries of the data, the mesh used in this study included `r mesh.gf$n` vertices (Figure \@ref(fig:Mesh)).

For the INLA models data up to `r max(dat.final$year)` were used, while survey data from 2017-2019 were excluded from the main analysis and used only as testing data for the spawning distribution case study.  For all analyses the response variable was presence absence of the species of interest ($EP_{it}$) and a *Bernoulli* GLM was utilized within R-INLA.

$$ EP_{it} \sim Bernoulli(\pi_{it}) $$

\begin{align}
  E(EP_{it}) = \pi_it  \qquad and \qquad  var(EP_it) = \pi_it \times (1-\pi_it)
\end{align}

$$ logit(pi_{it}) = \alpha + f(Cov_{it}) + u_{it} $$

$$ u_{it} \sim GMRF(0,\Sigma) $$

Each variable retained after the VIF analysis along with the 4 PCA components were added to the model individually, all continuous covariates were modelled using the INLA random walk $'rw2'$ smoother which allows for non-linear relationships between the response and each variable (Cite ZUUR Vol 1). The continuous covariates were centred at their mean value and scaled by their standard deviation, covariates which were highly skewed (e.g. depth) were log transformed before being standardized. Due to low sample size of several of the levels the Sediment type [Sed ; data obtained from @mcmullen2014GISData2014] were amalgamated into one factor level which was represented by the 'intercept' term  in models which included the Sediment type. This amalgamated level represented approximately `r 100-per.3.4.sed`% of survey tows across the three surveys (approximately `r per.3.4.sed`% of the survey tows were on the Sand or Gravel-Sand bottoms).

Four spatial random fields ($u_{it}$) were compared for each species and each survey, these included a) a static random field (t = 1), b) independent random fields every 10 years, c) independent random fields every 5 years, and d) and independent random fields every 3 years.  For b-d the random fields were set from the most recent year, so that when the time series was not a multiple of the time series length the first years of data had a shorter duration random field (e.g. the 10 year random fields for NMFS-spring survey were 2007-2016, 1997-2006, 1987-1996, 1977-1986, and 1972-1976). Models with the same covariate structure but different random fields were compared using WAIC, CPO, and DIC; the results for each of these metrics were similar and only the WAIC results are discussed further.  In all cases the static spatial field was an inferior model when compared to models with multiple random fields and the results discussed here use the 10/5/3 year random fields.

Initial model selection for the different covariate models was undertaken using a static random field (due to computational constraints) by adding individual covariates. For this first analysis covariates were retained if low WAIC (CPO and DIC results were similar to WAIC so only WAIC is discussed further) scores were observed across multiple models. For cod this analysis identified depth (DEP) and the average sea surface temperature between 1997 and 2008 (SST) as having low WAIC scores in 2 of the 3 surveys [Data obtained from @greenlawGeodatabaseHistoricalContemporary2010].  For yellowtail, depth (DEP) was the primary covariate observed, in addition sediment grain size (SED), and the average chlorophyll concentration between 1997 and 2008 (CHL) were retained due to their low scores in one survey. These variables were added pairwise (e.g. models included SST + DEP, DEP + CHL, and SST + CHL) for both species and again compared using WAIC. For cod a three term model including additive terms for SST, DEP, and CHL was the most complex model tested, while for yellowtail the most complex model included SST, DEP, and SED. For this step additional covariates were retained if the WAIC for that model resulted in an improvement of the WAIC of more than 2 when compared to the lowest WAIC more parsimonious model.

### Model Validation 

Five fold cross validation was used to test the predictive performance of the models. The data were randomly divided into 5 subsets and trained using 4 of the subsets, the 5th dataset was treated as a testing dataset to determine how well the model was able to predict out of sample data. Model performance was measured by comparing the the model residuals from the training data to the prediction error from the testing data, the metrics used for this comparison were Root Mean Squared Error (RMSE), Mean Average Error (MAE), and the standard deviation (SD).  For computational reasons the models compared using 5 fold cross validation were intercept only, SST (cod), DEP (yellowtail), DEP + SST, the 5 year random field was used for all model validation for both species.


# Results

### Model Selection

Initial model selection resulted in a significant reduction in the number of covariates in the model.  For cod, the Winter (DFO) and Spring (NMFS) both identified SST as significant covariates, while the Spring survey also identified depth and stratification, the Fall (NMFS) survey did not indicate any covariates with an WAIC that were a significant improvement from the intercept only model, although again the inclusion of Depth did result in a slightly smaller WAIC (Figure \@ref(fig:diag-1-fe)).  For yellowtail, inclusion of depth significantly improved the models in all 3 seasons (surveys), while Sediment type (Sed) and chlorophyll concentration in the Fall had a similar impact on the model WAIC as SST. As a result SST, Depth, Chl, and Sed were used to explore the development of more complex covariate models. For cod these more complex models resulted in an additive Dep + SST model being the preferred model in all 3 seasons (Figure \@ref(fig:diag-2-fe)). For yellowtail the best models with 2 covariates included some combination of Dep, SST, and Sed, further model selection indicated that the best model for yellowtail in all 3 seasons was an additive model including Dep, SST, and Sed (Figures \@ref(fig:diag-2-fe) and \@ref(fig:diag-3-fe)). The cod the 5 year random field had the lowest WAIC in all seasons, while for yellowtail the 3 year field was preferred for Winter and Spring, while the 5 year field was preferred for Fall (Figure \@ref(fig:diag-rf)).

### Environmental Variables

The spatial fields for the three environmental variables retained by model selection are shown in Figure \@ref(fig:SST-Dep-Sed).  The average SST between 1997 and 2008 had the largest effect on the EP (encounter probability) of cod, they were generally more likely to be found in regions of the bank with a lower SST. For all 3 surveys the EP declined rapidly in regions of the bank in which the SST was above approximately 10°C (Figure \@ref(fig:cod-fe)).  The depth relationship was also retained in the final cod model though the effect on EP was substantially smaller, during the Winter and Spring the EP peaked between `r c.dep.peak` meters and declined slowly in shallower and deeper waters (Figure \@ref(fig:cod-fe)).

For yellowtail depth had the largest effect on EP, with Yellowtail being most likely to be observed between depths of `r yt.dep.peak` meters in each of the 3 surveys and the EP being highest for this effect during the Spring (Figure \@ref(fig:yt-fe)). The average SST between 1997 and 2008 was also included in the final model in all three seasons, with yellowtail EP generally declining slightly as SST increased.  The sediment type also had a significant influence on the EP for yellowtail, with Sand and Gravel-Sand having higher EP's than other sediment types, this difference is most notable during the Winter, but model selection slightly favoured the Sediment model in the Spring and Winter as well although the effect size declined in these years (Figures \@ref(fig:cod-fe) and \@ref(fig:diag-3-fe)). 

Inter-annual and Seasonal Variability 

For both species the distribution shifted towards the north and east throughout the study period (Figure \@ref(fig:cog-hep)). For cod the shift in distribution occurred relatively rapidly in the 1990s and the center of gravity has been relatively stable since this period (Figure \@ref(fig:cog-hep)).  This shift in distribution of cod has largely occurred due to the loss areas with high encounter probabilities on the U.S. side of the bank (Fig Supplement), the center of gravity of the population has been well within Canadian waters since this shift for all 3 surveys. In addition, the fall survey indicates that cod has tended to be distributed along the northern edge of GB and the distribution of cod during this time likely includes the northern slope of the bank where there is limited survey coverage. The area of high encounter probability has followed a similar temporal pattern as the distribution, with a rapid decline in the area of high encounter probability for cod occurring in the 1990s in the winter and spring ((Figure \@ref(fig:area-hep))). In the fall the decline in high encounter probability (HEP) was observed approximately a decade earlier than in the winter or spring, the area of HEP has been much smaller during the fall, given the location of the stock along the edge of the bank during this period it is likely that a substantial portion of the population is located along the slope where survey coverage is limited ((Figure \@ref(fig:area-hep) and Supplement).

The yellowtail shift in distribution has in large part been due to a loss of HEP along the southern flanks of GB and the region of HEP has been consolidated in a central region of GB which straddles the ICJ line dividing Canada and the U.S (Figure \@ref(fig:cog-hep) and Supplement). This center of gravity of yellowtail has been very stable both seasonally and inter-annually since the 1990s despite large changes in the HEP area  during this time.  The trends in and size of the HEP area for the Spring and Fall surveys have been very similar since the 1980s with large increases in HEP area in the 1990s followed by variable yet increasing HEP area ((Figure \@ref(fig:area-hep)). The Winter survey identifies an area of similar location and size, but the Winter HEP trend has been in decline since a period of increase in the 1990s ((Figure \@ref(fig:area-hep)).

### Validation and Prediction

The 5-fold cross validation indicated that all of the models were able to predict the distribution for all species and surveys without a significant loss of accuracy, the mean error of the residuals for the validation training set predictions were similar to the error from the predicted test data, although the mean error of the test data were generally more variable ((Figure \@ref(fig:folds)).  The RMSE from the test and training data showed similar patterns for both species and most of the models, although notably the RMSE from the intercept only Yellowtail model was somewhat lower than either of the models with covariates indicating that the the inclusion of explicit covariates may result in a small loss of out of sample prediction for this species ((Figure \@ref(fig:folds)).


The 3 and 5 year random field models resulted in a loss of accuracy when predicting the spawning distributions of each species 1, 2, and 3 years into the future (Figure \@ref(fig:pred-17-19)), but the predictions were well below the RMSE associated with a model with no predictive ability (dashed line Figure \@ref(fig:pred-17-19)).  For both species the 2018 data consistently had the lowest prediction accuracy with the predictive models tending expect encounters where no individuals were observed. (Figure \@ref(fig:pred-17-19) and Supplement).



# Discussion

Here we have shown how models which incorporate environmental, spatial, and multi-scale temporal information can be used to partition static environmental relationships from dynamic changes which occur both inter and intra-annually. This framework enables a better understanding of the magnitude of dynamical shifts along with identifying regions of consistently high and low probability of encounter throughout the study region. The results indicate that few of the static environmental covariates related to groundfish distribution with only a static SST layer, depth, and sediment type having any consistent relationship to the likelihood of encountering either species throughout the duration of this study. A general shift in the distribution of both species towards the east and north was identified, in both cases this shift was in large part due to the loss of high EP areas in the southern and western portion of GB. In addition, the analysis of surveys from different times of the year provided a snapshot of the seasonal changes in the distributions of the species; we observed that the yellowtail distribution is relatively stable throughout the year, while cod move towards the slope of GB during the fall. The models  were able to predict the location of spawning cod and yellowtail up to 3 years in the future with only a modest loss of predictive.  Finally, the models were used to better understand how the overlap between the closed areas and the high EP areas during spawning for both species has changed over time in both the U.S. and Canada.

There is good stuff in the below but I need to pivot these ideas to the long term trends and the differences between Canada and the US.
<!-- ### Yellowtail -->

<!-- In the U.S. portion of GB closures were put in place in 1994 to assist with the rebuilding of stocks in the region, these closures have been considered as instrumental in the rebuilding of several stocks in the late 1990s [@linkEffectsAreaClosures2005; @murawskiLargescaleClosedAreas2000].  Here we see a slight increase in the high EP area for yellowtail during spawning in CA I around the time the closure was put in place which followed a steady decline in the 1970s and 1980s. The high EP area has subsequently declined steadily for yellowtail during its spawning period in CA I.  While CA I historically had represented less than XXX of high EP yellowtail area, in recent decades this has dropped to near 0. Given the restrictions on fishing activity inside CA I during this period, it is likely that this shift in the distribution is due the shifting environmental conditions on GB [@allynComparingSynthesizingQuantitative2020]. Closed Area II (CA II), which straddles the ICJ line, had experienced a large rapid decline in high EP yellowtail area during spawning in the years leading up to the implementation of the CA II closure. This was followed by an large rapid increase in high EP for yellowtail when CA II was put in place.  In recent years CA II has contained a substantial proportion of the high EP yellowtail area on GB during spawning and appears to represent the last large scale habitat suitable for yellowtail on the U.S. side of GB. The rapid expansion of the core yellowtail habitat in the early 2000's was centred on CA II with spillover evident into Canada and corresponded to a rapid increase in yellowtail biomass on GB [@legaultStockAssessmentGeorges2018].  The core area of high EP for yellowtail remained relatively stable starting in the early 2000s and was similar in size to what was observed in the 1970s before this closure was put in place. While these results suggest evidence of a positive association between this closure and yellowtail status, the abrupt yellowtail population decline in the early 2010s [@legaultStockAssessmentGeorges2018], despite the ongoing minimal fishing effort in the area, suggest that the shifting environmental conditions on GB may now be effecting the stock within CA II [@pershingSlowAdaptationFace2015].  -->

<!-- On the Canadian side of GB (approximated here by the domain of the COSF) there has been no directed yellowtail fishery since 2012 [@legaultStockAssessmentGeorges2018]. The primary source of fishery mortality comes from bycatch in the Canadian groundfish and COSF fisheries. In an effort to protect these spawning aggregations from bycatch from these fisheries on GB the Canadian groundfish fishery is excluded from GB from early February until the end of May while the COSF is excluded from fishing inside the time-area closure analyzed in this study in June. Unfortunately, this time-area closure protects only a small proportion of the high EP yellowtail spawning area within the COSF fishing domain. While the area in which the COSF is excluded from are predominately regions in which yellowtail are commonly found, due to their limited size these closures are likely to have little impact on bycatch in the COSF. This aligns with previous research which found that bycatch rates from the COSF remain elevated when this closure is in place [Cite PLOS-one]. -->

<!-- The depth, static SST, and sediment type were generally the most influential variables for yellowtail for all the models tested. Yellowtail was unlikely to be found on bottom types which did not include sand and was more frequently found at depths between XXX meters [@johnsonYellowtailFlounderLimanda1999] and in historically lower SST regions of the bank; most of the remaining habitat which meet these criteria are found in CA II and in Canadian waters on GB. The random fields in each season (see supplemental material) also indicated a consistent increased likelihood of encountering yellowtail in the region straddling the Canadian and U.S. border which suggests there is some unexplained ecological or environmental significance in this region. The shift in the distribution of yellowtail away from more southern and western parts of GB combined with the declines in biomass of yellowtail throughout the U.S. supports the view that the environmental change which has been observed throughout U.S. waters has been a factor in the recent decline of yellowtail both on GB and throughout the region [@legaultStockAssessmentGeorges2018; @nfsc54thNortheastRegional2012; @NOAAYellowtailFlounder2020; @pershingSlowAdaptationFace2015]. Given the loss of Yellowtail from the warmer portions of the bank observed in this study it is possible that the remaining core area around CA II and in Canada represents the most northern suitable habitat on GB for this species. If temperatures continue to increase as projected the suitability of this habitat may decline which would increase the risk of extirpation of yellowtail from GB irrespective of any fisheries management action [@allynComparingSynthesizingQuantitative2020]. -->

<!-- ### Cod -->

<!-- On the U.S. side of GB there was a rapid decline in the high EP area within CA I for cod during spawning which continued even after this area was closed. This decline was similar to the decline observed at the bank scale and in recent years there has been no high EP area within CA I.  Similar rapid declines were observed in CA II and since 1987 this represents a loss of over XXX km² of high EP area in the U.S. despite the implementation of these closures. These two closures represented approximately XXX% of the high EP cod area during spawning in the late 1980s but only XXX% of the high EP spawning area on GB in more recent eras. These declines are similar to what has been experience throughout the U.S. side of GB during this time, with a loss of cod from U.S. waters and a shift in the distribution of the species to be predominately located in Canadian waters throughout the year. -->

<!-- On the Canadian side of GB (approximated here by the domain of the COSF) while there has been a large decline in high EP area, this decline has been far more muted than experienced in U.S. waters. Declines in Canada peaked at approximately XXX% in the COSF domain in recent years. Combined with the losses observed in the U.S. this resulted in a rapid increase in the proportion of the high EP domain being located in Canada; the COSF domain accounted for only XXX % of high EP during spawning in the late 1980s, but in the most recent era this had increased to XXX% of the high EP area for cod was located with the COSF domain. Similar to yellowtail, the closure of the COSF to protect spawning aggregations of cod are predominately located in high EP areas, but due to their limited size the closures protect only a small percentage of the high cod EP area within the COSF domain. This agrees with evidence that bycatch rates remain elevated when this closure is in place due to the small size of these closures with respect to the area of the COSF [Cite PLOS-one]. -->

<!-- For cod the static SST layer and depth were the most influential covariates and indicated that cod preferred the colder portions of the bank throughout the year. The distribution of cod has steadily shifted throughout the duration of the study period. While the depth preference of cod is more variable than yellowtail [@fahayAtlanticCodGadus1999; @johnsonYellowtailFlounderLimanda1999], as observed with yellowtail, the loss of high EP areas in the more southern and western reaches of the bank have primarily been the reason for the apparent shift in the distribution of cod into Canadian waters.  More advanced models using either a dynamic SST or modelled bottom temperature layer would lead to further insights into how changes in the thermal environment have influenced the distribution of cod over time [@greenanClimateChangeVulnerability2019; @pershingSlowAdaptationFace2015]. The interpretation of the static SST layer used in these analyses as thermal effect is likely somewhat unrealistic as it assumes that the relative temperature patterns and the species reaction to these patterns have remained static over the study period.   -->

<!-- The high EP area for cod collapsed rapidly in the early 1990's in unison with the collapse of cod (and other groundfish) stocks throughout the Northwest Atlantic [@bundySealsCodForage2009].  Since the collapse the core area has remained relatively consistent but has continued to slowly shift to the north and east throughout the year, though the shift is more pronounced in the fall. The fall distribution of cod is likely now located on the northeastern slope of the bank outside of the core survey domains of any of the surveys.  This northeastern shift of the population over the course of this study suggests that the surveys are no longer sampling the entirety of this population throughout the course of the year (i.e. a higher proportion of the stock is now located outside of this area).  Each of the survey indices area used as inputs to the cod stock assessment model for eastern GB cod [@andrushchenkoAssessmentEasternGeorges2018]. This assessment model suffered from such significant retrospective patterns that this stock assessment model was eventually rejected; it is possible that the observed shift in the distribution of cod outside of the survey domain was an unknown contributing factor to the model retrospective problems [@andrushchenkoAssessmentEasternGeorges2018]. In addition, because the management of this stock is shared between Canada and the U.S., the observed shift in the core distribution to Canadian waters suggests that shared management policies, such as quota sharing agreements between the two jurisdictions, may require regular review [e.g. @tmgcDevelopmentSharingAllocation2002]. -->

# Conclusions

These models provide insight into how the distribution of both species changes both seasonally and inter-annually and how many environmental covariates, when treated spatially, have little impact on these patterns. The only static environmental data which had a significant effect on the species distributions were the average sea surface temperature (1997-2008), depth, and bottom type (yellowtail only). The inter-annual shifts in species distribution indicate the increasing importance of Canadian waters for both species on GB which is likely is due to the long-term environmental shifts observed in the region. The distributional shifts also highlight the difficulties encountered by static closed areas in light of environmental change, for both species the utilization of the closed areas has changed over time and for CA I it is likely this area not longer contains substantial areas of core habitat for either of these species in the current environment at any time of the year. Given the habitat constraints faced by both species the continuation of directed environmental change will likely put both species at increased risk of extirpation from U.S. waters and, in the longer term, all of GB irrespective of any fisheries management action. The utilization of the spatio-temporal information contained in these models provides novel insights which can be used to improve science advice (e.g. accounting for shifting distributions in stock assessment and choosing the location of protected areas) and lead to more informed fisheries management decisions. 

## Acknowledgements 

Ms. Pectindy


\onecolumn


<!-- Insert table 1 note how I'm dealing with the figure caption here-->
```{r,envrio,echo=F}
options(knitr.kable.NA = '')
knitr::kable(
  table_1, booktabs = TRUE, format='pandoc',
  caption = "Enviromental variables used in the analysis "
)
```


<br>

```{r Overview, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Georges Bank (GB) study area.  Points represent the sample locations for each of the three surveys and the orange outline represets the core region of GB included in these analyses (42,000 km²).  In the U.S. the blue polygon is Closed Area I (CA I) and the white polygon is Closed Area II (CA II). In Canada the small gold bordered cells (each cells covers an area of approximately 42.7 km²) represent areas which have been included in either the cod or yellowtail closures at least once.  Some of the cells have been part of both closures and not all cells are closed each year; darker fill indicates cells which have been closed more frequently. The red line indicates the Canadian exclusive economic zone (EEZ)."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(over.plt)
```


```{r Mesh, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Delaunay triangular mesh used for the spatial fields mesh. The mesh contains 6610 vertices."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(mesh.plt)
```



```{r gini-index, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Gini Index "}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(gini.index.plt)
```

```{r diag-1-fe, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Initial stage of forward model selection using each of the environmental covariates individually.  This model selection was done using a static random field. Blue dashed line represents 2 WAIC units larger than the preferred model, the red dashed line is 10 WAIC units larger than the preferred model WAIC. "}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(diag.waic.single.fe.plt)
```

```{r diag-2-fe, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Stage 2 of model selection including additive models with 2 covariates based on the covariates identified in the initial model selection stage. These models were compared using the 10-year random field models. Blue dashed line represents 2 WAIC units larger than the preferred model, the red dashed line is 10 WAIC units larger than the preferred model WAIC."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(diag.waic.2.covars.fe.plt)
```

```{r diag-3-fe, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Final stage of covariate model selection which includes model with up to 3 covariate terms based on models selected at stage 2. Blue dashed line represents 2 WAIC units larger than the preferred model, the red dashed line is 10 WAIC units larger than the preferred model WAIC."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(diag.waic.3.covars.fe.plt)
```

```{r diag-rf, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Model selection comparing the random fields models.  For cod the model used is Dep + SST for all of the random fields.  For Yellowtail the 5 and 10 year random fields were compared using the Dep + SST model, while the 5 and 3 fields were compared using the slightly preferred Dep + SST + Sed model. Blue dashed line represents 2 WAIC units larger than the preferred model, the red dashed line is 10 WAIC units larger than the preferred model WAIC."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(diag.waic.rf.plt)
```

```{r SST-Dep-Sed, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Average Sea Surface Temperature on Georges Bank (GB) from 1997-2008 (SST in °C) in the top panel, GB bathymetry (depth in meters) in the center panel, and GB sediment type in the bottom panel."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(sst_depth_spatial.plt)
```

```{r cod-fe, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Fixed effects for cod from each survey, top row is the depth covariate effect, bottom row is the SST effect. Results transformed to the probability scale and the blue shaded region represents the 95% credible interval."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(cod.fe.plt)
```

```{r yt-fe, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Fixed effects for yellowtail from each survey, the top row is the depth covariate effect, middle row is the SST effect and the bottom row is the effect of sediment type. Results transformed to the probability scale, and the blue shaded region and the error bars represent the 95% credible intervals."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(yt.fe.plt)
```

```{r cog-hep, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Center of Gravity (COG) for the high EP areas for cod (top panel) and yellowtail (bottom panels) in the Winter (left), Spring (center), and Fall (right).  Blue lines indicate ±3 standard deviation units from the mean COG for each era using the 5 year random field models. Labels indicate the years associated with each era and the red line is border between the U.S. and Canada."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(cog.plt)
```

```{r area-hep, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Time series of the total area on GB classified as high EP for each of the three surveys.  The cod time series is on the left and the yellowtail on the right.  The black line represents the Winter trend, the Blue line is the Spring trend and the red line is the Fall trend.  "}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(area.plt)
```

```{r folds, echo=FALSE,out.width="100%",dpi=200,fig.cap = "Results of 5 fold cross validation analyses. Top panels represents the mean error for each of the three covariate models tested for cod and yellowtail. Blue points represent the prediction error from the testing dataset, while the black points are the residuals from the training dataset. The bottom panels are the Root Mean Squared Error (RMSE) for these models.  The dashed line represents the RMSE for randomly generated data and represents the RMSE for a model with no predictive ability."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(folds.plt)
```

```{r pred-17-19, echo=FALSE,out.width="100%",dpi=200,fig.cap = "The residual Root Mean Squared Error for the model in each year in black. The blue lines represent the prediction RMSE for years 2017, 2018, and 2019. The model used include the SST and Depth covariates, the cod results are in the top row and yellowtail in the bottom row and the results for the 3-year fields are on the left and the 5-year random field on the right. The dashed line represents the RMSE for randomly generated data and represents the RMSE for a model with no predictive ability."}
# Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output
knitr::include_graphics(pred.17.19.plt)
```


<!-- ```{r CA1-2, echo=FALSE,out.width="100%",dpi=200,fig.cap = "The high EP area located within the U.S. Closed Area I (CA I; top row) and Closed Area II (CA II; bottom row).  The panels on the left represents the total area of high EP by era. The panels on the right is the proporiton of the total high EP on GB which is located within the closure.  The green line represent cod and the black line represents yellowtail. "} -->
<!-- # Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output -->
<!-- knitr::include_graphics(CA1.CA2.plt) -->
<!-- ``` -->


<!-- ```{r scal-fa, echo=FALSE,out.width="100%",dpi=200,fig.cap = "The high EP area located within the Canadian Offshore Scallop Fishing (COSF) domain during spawning in each era.  The panel on the left represents the total area of high EP, the middle panel is the proportion of the total high EP area on GB found within the COSF domain.  The panel on the right is the proporiton of the COSF domain that is classified as high EP.  The green line represents cod and th e black line represents yellowtail. "} -->
<!-- # Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output -->
<!-- knitr::include_graphics(scal.FA.plt) -->
<!-- ``` -->

<!-- ```{r scal-close, echo=FALSE,out.width="100%",dpi=200,fig.cap = "The high EP area located within the Candian Offshore Scallop Fishery (COSF) cod and yellowtail closures during spawning for each species.  The panel on the left represents the total area of high EP by year for each closure, the middle panel is the proportion of the closure with a high EP.  The panel on the right is the proporiton of the total high EP within the COSF domain that is located within the closure.  The green line represents cod closure and the black line represents yellowtail. "} -->
<!-- # Note that out.width and out.height and dpi don't do anything for word document output in the chunck control above, they wil impact html and pdf output -->
<!-- knitr::include_graphics(scal.closures.plt) -->
<!-- ``` -->






\newpage
<br>

# References {-}

<div id="refs"></div>

\newpage 
